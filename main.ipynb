{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f080e00",
   "metadata": {},
   "source": [
    "# yolov5-face\n",
    "* YOLOv5是一种单阶段目标检测算法，在这个样例中，我们选取了YOLOv5s，它是YOLOv5系列中较为轻量的网络，适合在边缘设备部署，进行实时目标检测。\n",
    "\n",
    "# 前期准备\n",
    "* 基础镜像的样例目录中已包含转换后的om模型以及测试图片，如果直接运行，可跳过此步骤。如果需要重新转换模型，可以参考下面的步骤。\n",
    "* 首先我们可以在[这个链接](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Atlas%20200I%20DK%20A2/DevKit/downloads/23.0.RC1/Ascend-devkit_23.0.RC1_downloads.xlsx)的表格中找到本样例的依赖文件，下载我们已经准备好了的ONNX模型，ONNX是开源的离线推理模型框架。\n",
    "\n",
    "* 为了能进一步优化模型推理性能，我们需要将其转换为om模型进行使用，以下为转换指令：  \n",
    "    ```shell\n",
    "    atc --model=yolov5s.onnx --framework=5 --output=yolo --input_format=NCHW --input_shape=\"input_image:1,3,640,640\" --log=error --soc_version=Ascend310B1\n",
    "    ```\n",
    "    * 其中转换参数的含义为：  \n",
    "        * --model：输入模型路径\n",
    "        * --framework：原始网络模型框架类型，5表示ONNX\n",
    "        * --output：输出模型路径\n",
    "        * --input_format：输入Tensor的内存排列方式\n",
    "        * --input_shape：指定模型输入数据的shape\n",
    "        * --log：日志级别\n",
    "        * --soc_version：昇腾AI处理器型号\n",
    "        * --input_fp16_nodes：指定输入数据类型为FP16的输入节点名称\n",
    "        * --output_type：指定网络输出数据类型或指定某个输出节点的输出类型\n",
    "\n",
    "# 模型推理实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d7761be-840e-4fc8-b501-7a5f9a0a56b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# 导入代码依赖\n",
    "import cv2\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import torch\n",
    "from skvideo.io import vreader, FFmpegWriter\n",
    "import IPython.display\n",
    "from ais_bench.infer.interface import InferSession\n",
    "\n",
    "from det_utils import letterbox, scale_coords, nms, scale_coords_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e9292fc-f49c-410d-8bf3-08069171c2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, cfg, bgr2rgb=True):\n",
    "    \"\"\"图片预处理\"\"\"\n",
    "    img, scale_ratio, pad_size = letterbox(image, new_shape=cfg['input_shape'])\n",
    "    if bgr2rgb:\n",
    "        img = img[:, :, ::-1]\n",
    "    img = img.transpose(2, 0, 1)  # HWC2CHW\n",
    "    img = np.ascontiguousarray(img, dtype=np.float32)\n",
    "    img /= 255.0  # 关键修改：归一化 0-255 -> 0.0-1.0\n",
    "    return img, scale_ratio, pad_size\n",
    "\n",
    "\n",
    "def draw_bbox(bbox, img0, color, wt, names):\n",
    "    \"\"\"在图片上画预测框\"\"\"\n",
    "    det_result_str = ''\n",
    "    for idx in range(bbox.shape[0]):\n",
    "        # 关键修改：yolov5-face NMS后，index 5 是类别\n",
    "        class_id = int(bbox[idx][5]) \n",
    "        \n",
    "        # 关键修改：修复之前的语法错误 float(bool) -> float(val)\n",
    "        if float(bbox[idx][4]) < 0.05:\n",
    "            continue\n",
    "            \n",
    "        img0 = cv2.rectangle(img0, (int(bbox[idx][0]), int(bbox[idx][1])), (int(bbox[idx][2]), int(bbox[idx][3])),\n",
    "                             color, wt)\n",
    "        \n",
    "        label_name = names[int(class_id)] if names and len(names) > class_id else 'face'\n",
    "        img0 = cv2.putText(img0, str(idx) + ' ' + label_name, (int(bbox[idx][0]), int(bbox[idx][1] + 16)),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "        img0 = cv2.putText(img0, '{:.4f}'.format(bbox[idx][4]), (int(bbox[idx][0]), int(bbox[idx][1] + 32)),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "        \n",
    "        # 关键修改：关键点从 index 6 开始\n",
    "        if bbox.shape[1] >= 16:\n",
    "            for k in range(5):\n",
    "                kpt_x = int(bbox[idx][6 + 2 * k])\n",
    "                kpt_y = int(bbox[idx][6 + 2 * k + 1])\n",
    "                cv2.circle(img0, (kpt_x, kpt_y), 3, (255, 0, 0), -1)\n",
    "\n",
    "        det_result_str += '{} {} {} {} {} {}\\n'.format(\n",
    "           label_name, str(bbox[idx][4]), bbox[idx][0], bbox[idx][1], bbox[idx][2], bbox[idx][3])\n",
    "    return img0\n",
    "\n",
    "\n",
    "def get_labels_from_txt(path):\n",
    "    # ...existing code...\n",
    "    labels_dict = dict()\n",
    "    with open(path) as f:\n",
    "        for cat_id, label in enumerate(f.readlines()):\n",
    "            labels_dict[cat_id] = label.strip()\n",
    "    return labels_dict\n",
    "\n",
    "\n",
    "def draw_prediction(pred, image, labels):\n",
    "    # ...existing code...\n",
    "    imgbox = widgets.Image(format='jpg', height=720, width=1280)\n",
    "    img_dw = draw_bbox(pred, image, (0, 255, 0), 2, labels)\n",
    "    imgbox.value = cv2.imencode('.jpg', img_dw)[1].tobytes()\n",
    "    display(imgbox)\n",
    "\n",
    "\n",
    "def infer_image(img_path, model, class_names, cfg):\n",
    "    \"\"\"图片推理\"\"\"\n",
    "    image = cv2.imread(img_path)\n",
    "    img, scale_ratio, pad_size = preprocess_image(image, cfg)\n",
    "    \n",
    "    # 关键修改：增加 Batch 维度 (3,640,640) -> (1,3,640,640)\n",
    "    if len(img.shape) == 3:\n",
    "        img = img[None] \n",
    "\n",
    "    output = model.infer([img])[0]\n",
    "    output = torch.tensor(output)\n",
    "    \n",
    "    # NMS nm=10 保留关键点\n",
    "    boxout = nms(output, conf_thres=cfg[\"conf_thres\"], iou_thres=cfg[\"iou_thres\"], nm=10)\n",
    "    pred_all = boxout[0].numpy()\n",
    "    \n",
    "    # 坐标还原\n",
    "    scale_coords(cfg['input_shape'], pred_all, image.shape, ratio_pad=(scale_ratio, pad_size))\n",
    "    if pred_all.shape[1] >= 16:\n",
    "        # 关键点还原，传入 index 6:16\n",
    "        pred_all[:, 6:16] = scale_coords_landmarks(cfg['input_shape'], pred_all[:, 6:16], image.shape, ratio_pad=(scale_ratio, pad_size))\n",
    "        \n",
    "    draw_prediction(pred_all, image, class_names)\n",
    "\n",
    "\n",
    "def infer_frame_with_vis(image, model, labels_dict, cfg, bgr2rgb=True):\n",
    "    img, scale_ratio, pad_size = preprocess_image(image, cfg, bgr2rgb)\n",
    "    \n",
    "    # 关键修改：增加 Batch 维度\n",
    "    if len(img.shape) == 3:\n",
    "        img = img[None]\n",
    "\n",
    "    output = model.infer([img])[0]\n",
    "    output = torch.tensor(output)\n",
    "    \n",
    "    boxout = nms(output, conf_thres=cfg[\"conf_thres\"], iou_thres=cfg[\"iou_thres\"], nm=10)\n",
    "    pred_all = boxout[0].numpy()\n",
    "    \n",
    "    scale_coords(cfg['input_shape'], pred_all, image.shape, ratio_pad=(scale_ratio, pad_size))\n",
    "    if pred_all.shape[1] >= 16:\n",
    "        pred_all[:, 6:16] = scale_coords_landmarks(cfg['input_shape'], pred_all[:, 6:16], image.shape, ratio_pad=(scale_ratio, pad_size))\n",
    "        \n",
    "    img_vis = draw_bbox(pred_all, image, (0, 255, 0), 2, labels_dict)\n",
    "    return img_vis\n",
    "\n",
    "\n",
    "def img2bytes(image):\n",
    "    \"\"\"将图片转换为字节码\"\"\"\n",
    "    return bytes(cv2.imencode('.jpg', image)[1])\n",
    "\n",
    "\n",
    "def infer_video(video_path, model, labels_dict, cfg):\n",
    "    \"\"\"视频推理\"\"\"\n",
    "    image_widget = widgets.Image(format='jpeg', width=800, height=600)\n",
    "    display(image_widget)\n",
    "\n",
    "    # 读入视频\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while True:\n",
    "        ret, img_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # 对视频帧进行推理\n",
    "        image_pred = infer_frame_with_vis(img_frame, model, labels_dict, cfg, bgr2rgb=True)\n",
    "        image_widget.value = img2bytes(image_pred)\n",
    "\n",
    "\n",
    "def infer_camera(model, labels_dict, cfg):\n",
    "    \"\"\"外设摄像头实时推理\"\"\"\n",
    "    def find_camera_index():\n",
    "        max_index_to_check = 10  # Maximum index to check for camera\n",
    "\n",
    "        for index in range(max_index_to_check):\n",
    "            cap = cv2.VideoCapture(index)\n",
    "            if cap.read()[0]:\n",
    "                cap.release()\n",
    "                return index\n",
    "\n",
    "        # If no camera is found\n",
    "        raise ValueError(\"No camera found.\")\n",
    "\n",
    "    # 获取摄像头\n",
    "    camera_index = find_camera_index()\n",
    "    cap = cv2.VideoCapture(camera_index)\n",
    "    # 初始化可视化对象\n",
    "    image_widget = widgets.Image(format='jpeg', width=1280, height=720)\n",
    "    display(image_widget)\n",
    "    while True:\n",
    "        # 对摄像头每一帧进行推理和可视化\n",
    "        _, img_frame = cap.read()\n",
    "        image_pred = infer_frame_with_vis(img_frame, model, labels_dict, cfg)\n",
    "        image_widget.value = img2bytes(image_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e455e8d",
   "metadata": {},
   "source": [
    "# 样例运行\n",
    "\n",
    "* 初始化相关参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dccded13-fad1-4b88-b63f-d46deb23aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'conf_thres': 0.4,  # 模型置信度阈值，阈值越低，得到的预测框越多\n",
    "    'iou_thres': 0.5,  # IOU阈值，高于这个阈值的重叠预测框会被过滤掉\n",
    "    'input_shape': [640, 640],  # 模型输入尺寸\n",
    "}\n",
    "\n",
    "model_path = 'yolo_face.om'\n",
    "label_path = './coco_names.txt'\n",
    "# 初始化推理模型\n",
    "model = InferSession(0, model_path)\n",
    "labels_dict = {0: 'face'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e9cf34",
   "metadata": {},
   "source": [
    "* 选择推理模式。\"infer_mode\"有三个取值：image, camera, video，分别对应图片推理、摄像头实时推理和视频推理。默认使用视频推理模式。\n",
    "* 我们选取的样例是一个赛车视频，执行下面的代码后可以看到模型会对视频的每一帧进行推理，并将预测结果展示在画面上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aab68b6b-7d65-4dd9-ab0b-c5c9733c415a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce1d55043f842a69f1a4b12bbe86de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpeg', height='720', width='1280')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4187860/3350929844.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minfer_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0minfer_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'camera'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0minfer_camera\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0minfer_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'video'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvideo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'racing.mp4'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4187860/3522527725.py\u001b[0m in \u001b[0;36minfer_camera\u001b[0;34m(model, labels_dict, cfg)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;31m# 对摄像头每一帧进行推理和可视化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mimage_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer_frame_with_vis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mimage_widget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg2bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4187860/3522527725.py\u001b[0m in \u001b[0;36minfer_frame_with_vis\u001b[0;34m(image, model, labels_dict, cfg, bgr2rgb)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/lib/python3.9/site-packages/ais_bench/infer/interface.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(self, feeds, mode, custom_sizes)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'dymdims'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_dynamic_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdyshapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMemorySummary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/lib/python3.9/site-packages/ais_bench/infer/interface.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feeds, out_array)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout_array\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# convert to host tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "infer_mode = 'camera'  ## 如果修改为'camera' 则使用摄像头进行推理\n",
    "\n",
    "if infer_mode == 'image':\n",
    "    img_path = 'ImageforInfer.jpg'\n",
    "    infer_image(img_path, model, labels_dict, cfg)\n",
    "elif infer_mode == 'camera':\n",
    "    infer_camera(model, labels_dict, cfg)\n",
    "elif infer_mode == 'video':\n",
    "    video_path = 'racing.mp4'\n",
    "    infer_video(video_path, model, labels_dict, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27887b77",
   "metadata": {},
   "source": [
    "# 样例总结与扩展\n",
    "以上就是这个样例的全部内容了，值得关注的是在模型推理后有一步非常重要的后处理，就是非极大值抑制，即NMS，由于模型的原始预测结果会有非常多无效或重叠的预测框，我们需要通过NMS来进行过滤。再者，模型预测框的表示往往是一个标准化的结果，比如0到1之间，我们需要通过坐标转换将结果与原始图片的宽高对应上。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

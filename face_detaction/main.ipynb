{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f080e00",
   "metadata": {},
   "source": [
    "# yolov5-face + DeepSort\n",
    "* YOLOv5是一种单阶段目标检测算法，在这个样例中，我们选取了YOLOv5s，它是YOLOv5系列中较为轻量的网络，适合在边缘设备部署，进行实时目标检测。\n",
    "\n",
    "# 前期准备\n",
    "* 基础镜像的样例目录中已包含转换后的om模型以及测试图片，如果直接运行，可跳过此步骤。如果需要重新转换模型，可以参考下面的步骤。\n",
    "* 首先我们可以在[这个链接](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Atlas%20200I%20DK%20A2/DevKit/downloads/23.0.RC1/Ascend-devkit_23.0.RC1_downloads.xlsx)的表格中找到本样例的依赖文件，下载我们已经准备好了的ONNX模型，ONNX是开源的离线推理模型框架。\n",
    "\n",
    "* 为了能进一步优化模型推理性能，我们需要将其转换为om模型进行使用，以下为转换指令：  \n",
    "    ```shell\n",
    "    atc --model=yolov5s.onnx --framework=5 --output=yolo --input_format=NCHW --input_shape=\"input_image:1,3,640,640\" --log=error --soc_version=Ascend310B1\n",
    "    ```\n",
    "    * 其中转换参数的含义为：  \n",
    "        * --model：输入模型路径\n",
    "        * --framework：原始网络模型框架类型，5表示ONNX\n",
    "        * --output：输出模型路径\n",
    "        * --input_format：输入Tensor的内存排列方式\n",
    "        * --input_shape：指定模型输入数据的shape\n",
    "        * --log：日志级别\n",
    "        * --soc_version：昇腾AI处理器型号\n",
    "        * --input_fp16_nodes：指定输入数据类型为FP16的输入节点名称\n",
    "        * --output_type：指定网络输出数据类型或指定某个输出节点的输出类型\n",
    "\n",
    "# 模型推理实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7761be-840e-4fc8-b501-7a5f9a0a56b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# 导入代码依赖\n",
    "import cv2\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import torch\n",
    "from skvideo.io import vreader, FFmpegWriter\n",
    "import IPython.display\n",
    "from ais_bench.infer.interface import InferSession\n",
    "\n",
    "from det_utils import letterbox, scale_coords, nms, scale_coords_landmarks\n",
    "\n",
    "from deep_sort import nn_matching\n",
    "from deep_sort.detection import Detection\n",
    "from deep_sort.tracker import Tracker\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e744a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSort:\n",
    "    def __init__(self, model_path, max_dist=0.2, min_confidence=0.3, \n",
    "                 nms_max_overlap=1.0, max_iou_distance=0.7, \n",
    "                 max_age=70, n_init=3, nn_budget=100, use_cuda=False):\n",
    "        \n",
    "        self.session = InferSession(0, model_path)\n",
    "        \n",
    "        # 1. 自动解析模型输入\n",
    "        try:\n",
    "            input_desc = self.session.get_inputs()[0]\n",
    "            self.model_shape = tuple(input_desc.shape)\n",
    "            \n",
    "            shape_size = np.prod(self.model_shape)\n",
    "            if input_desc.size == shape_size:\n",
    "                self.model_dtype = np.uint8\n",
    "            else:\n",
    "                self.model_dtype = np.float32\n",
    "                \n",
    "        except:\n",
    "            self.model_shape = (16, 128, 64, 3)\n",
    "            self.model_dtype = np.float32\n",
    "\n",
    "        self.batch_size = self.model_shape[0]\n",
    "        \n",
    "        # 2. 解析 Layout (NHWC vs NCHW)\n",
    "        if len(self.model_shape) == 4 and self.model_shape[3] == 3:\n",
    "            self.layout = 'NHWC'\n",
    "            self.input_h = self.model_shape[1]\n",
    "            self.input_w = self.model_shape[2]\n",
    "        elif len(self.model_shape) == 4 and self.model_shape[1] == 3:\n",
    "            self.layout = 'NCHW'\n",
    "            self.input_h = self.model_shape[2]\n",
    "            self.input_w = self.model_shape[3]\n",
    "        else:\n",
    "            self.layout = 'NCHW'\n",
    "            self.input_h = 128\n",
    "            self.input_w = 64\n",
    "            \n",
    "        metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_dist, nn_budget)\n",
    "        self.tracker = Tracker(metric, max_iou_distance=max_iou_distance, max_age=max_age, n_init=n_init)\n",
    "        self.min_confidence = min_confidence\n",
    "\n",
    "    def _preprocess_batch(self, im_crops):\n",
    "        batch_data = np.zeros(self.model_shape, dtype=self.model_dtype)\n",
    "        \n",
    "        for i, crop in enumerate(im_crops):\n",
    "            if i >= self.batch_size: break\n",
    "            \n",
    "            # Resize\n",
    "            crop = cv2.resize(crop, (self.input_w, self.input_h))\n",
    "            \n",
    "            # BGR -> RGB\n",
    "            crop = crop[:, :, ::-1]\n",
    "            \n",
    "            # Layout 转换\n",
    "            if self.layout == 'NCHW':\n",
    "                crop = crop.transpose(2, 0, 1)\n",
    "            \n",
    "            if self.model_dtype == np.uint8:\n",
    "                # 模型需要 uint8，保持 0-255\n",
    "                crop = crop.astype(np.uint8)\n",
    "            else:\n",
    "                # 模型需要 float32，归一化到 0.0-1.0\n",
    "                crop = crop.astype(np.float32) / 255.0\n",
    "            \n",
    "            batch_data[i] = crop\n",
    "            \n",
    "        return np.ascontiguousarray(batch_data)\n",
    "\n",
    "    def update(self, bbox_xywh, confidences, classes, ori_img):\n",
    "        self.height, self.width = ori_img.shape[:2]\n",
    "        \n",
    "        features = []\n",
    "        if isinstance(confidences, torch.Tensor): confidences = confidences.cpu().numpy()\n",
    "        if isinstance(bbox_xywh, torch.Tensor): bbox_xywh = bbox_xywh.cpu().numpy()\n",
    "            \n",
    "        indices = [i for i, c in enumerate(confidences) if c > self.min_confidence]\n",
    "        bbox_xywh = bbox_xywh[indices]\n",
    "        confidences = confidences[indices]\n",
    "        \n",
    "        if len(bbox_xywh) > 0:\n",
    "            crops = []\n",
    "            for box in bbox_xywh:\n",
    "                x, y, w, h = box\n",
    "                x1, y1 = max(0, int(x)), max(0, int(y))\n",
    "                x2, y2 = min(self.width, int(x+w)), min(self.height, int(y+h))\n",
    "                crop = ori_img[y1:y2, x1:x2]\n",
    "                if crop.size == 0: continue\n",
    "                crops.append(crop)\n",
    "            \n",
    "            if crops:\n",
    "                num_crops = len(crops)\n",
    "                num_batches = int(np.ceil(num_crops / self.batch_size))\n",
    "                batch_features = []\n",
    "                \n",
    "                for b in range(num_batches):\n",
    "                    start = b * self.batch_size\n",
    "                    end = min((b + 1) * self.batch_size, num_crops)\n",
    "                    input_data = self._preprocess_batch(crops[start:end])\n",
    "                    output = self.session.infer([input_data.copy()])[0]\n",
    "                    batch_features.append(output[:end-start])\n",
    "                \n",
    "                if batch_features:\n",
    "                    features = np.vstack(batch_features)\n",
    "                else:\n",
    "                    features = np.array([])\n",
    "            else:\n",
    "                features = np.array([])\n",
    "        \n",
    "        detections = []\n",
    "        for i, (box, conf) in enumerate(zip(bbox_xywh, confidences)):\n",
    "            if i < len(features):\n",
    "                detections.append(Detection(box, conf, features[i].flatten()))\n",
    "\n",
    "        self.tracker.predict()\n",
    "        self.tracker.update(detections)\n",
    "\n",
    "        outputs = []\n",
    "        for track in self.tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1: continue\n",
    "            box = track.to_tlwh()\n",
    "            x1, y1, x2, y2 = int(box[0]), int(box[1]), int(box[0]+box[2]), int(box[1]+box[3])\n",
    "            outputs.append([x1, y1, x2, y2, track.track_id])\n",
    "            \n",
    "        return np.array(outputs)\n",
    "# ...existing code...# ...existing code...\n",
    "class DeepSort:\n",
    "    def __init__(self, model_path, max_dist=0.2, min_confidence=0.3, \n",
    "                 nms_max_overlap=1.0, max_iou_distance=0.7, \n",
    "                 max_age=70, n_init=3, nn_budget=100, use_cuda=False):\n",
    "        \n",
    "        self.session = InferSession(0, model_path)\n",
    "        \n",
    "        # 1. 自动解析模型输入\n",
    "        try:\n",
    "            input_desc = self.session.get_inputs()[0]\n",
    "            self.model_shape = tuple(input_desc.shape)\n",
    "            \n",
    "            # [关键修复] 自动判断模型需要 uint8 还是 float32\n",
    "            shape_size = np.prod(self.model_shape)\n",
    "            if input_desc.size == shape_size:\n",
    "                self.model_dtype = np.uint8\n",
    "            else:\n",
    "                self.model_dtype = np.float32\n",
    "                \n",
    "        except:\n",
    "            self.model_shape = (16, 128, 64, 3)\n",
    "            self.model_dtype = np.float32\n",
    "\n",
    "        self.batch_size = self.model_shape[0]\n",
    "        \n",
    "        # 2. 解析 Layout (NHWC vs NCHW)\n",
    "        if len(self.model_shape) == 4 and self.model_shape[3] == 3:\n",
    "            self.layout = 'NHWC'\n",
    "            self.input_h = self.model_shape[1]\n",
    "            self.input_w = self.model_shape[2]\n",
    "        elif len(self.model_shape) == 4 and self.model_shape[1] == 3:\n",
    "            self.layout = 'NCHW'\n",
    "            self.input_h = self.model_shape[2]\n",
    "            self.input_w = self.model_shape[3]\n",
    "        else:\n",
    "            self.layout = 'NCHW'\n",
    "            self.input_h = 128\n",
    "            self.input_w = 64\n",
    "            \n",
    "        metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_dist, nn_budget)\n",
    "        self.tracker = Tracker(metric, max_iou_distance=max_iou_distance, max_age=max_age, n_init=n_init)\n",
    "        self.min_confidence = min_confidence\n",
    "\n",
    "    def _preprocess_batch(self, im_crops):\n",
    "        batch_data = np.zeros(self.model_shape, dtype=self.model_dtype)\n",
    "        \n",
    "        for i, crop in enumerate(im_crops):\n",
    "            if i >= self.batch_size: break\n",
    "            \n",
    "            # Resize\n",
    "            crop = cv2.resize(crop, (self.input_w, self.input_h))\n",
    "            \n",
    "            # BGR -> RGB\n",
    "            crop = crop[:, :, ::-1]\n",
    "            \n",
    "            # Layout 转换\n",
    "            if self.layout == 'NCHW':\n",
    "                crop = crop.transpose(2, 0, 1)\n",
    "            \n",
    "            # [关键修复] 根据模型类型决定是否归一化\n",
    "            if self.model_dtype == np.uint8:\n",
    "                # 模型需要 uint8，保持 0-255\n",
    "                crop = crop.astype(np.uint8)\n",
    "            else:\n",
    "                # 模型需要 float32，归一化到 0.0-1.0\n",
    "                crop = crop.astype(np.float32) / 255.0\n",
    "            \n",
    "            batch_data[i] = crop\n",
    "            \n",
    "        return np.ascontiguousarray(batch_data)\n",
    "\n",
    "    def update(self, bbox_xywh, confidences, classes, ori_img):\n",
    "        self.height, self.width = ori_img.shape[:2]\n",
    "        \n",
    "        features = []\n",
    "        if isinstance(confidences, torch.Tensor): confidences = confidences.cpu().numpy()\n",
    "        if isinstance(bbox_xywh, torch.Tensor): bbox_xywh = bbox_xywh.cpu().numpy()\n",
    "            \n",
    "        indices = [i for i, c in enumerate(confidences) if c > self.min_confidence]\n",
    "        bbox_xywh = bbox_xywh[indices]\n",
    "        confidences = confidences[indices]\n",
    "        \n",
    "        if len(bbox_xywh) > 0:\n",
    "            crops = []\n",
    "            for box in bbox_xywh:\n",
    "                x, y, w, h = box\n",
    "                x1, y1 = max(0, int(x)), max(0, int(y))\n",
    "                x2, y2 = min(self.width, int(x+w)), min(self.height, int(y+h))\n",
    "                crop = ori_img[y1:y2, x1:x2]\n",
    "                if crop.size == 0: continue\n",
    "                crops.append(crop)\n",
    "            \n",
    "            if crops:\n",
    "                num_crops = len(crops)\n",
    "                num_batches = int(np.ceil(num_crops / self.batch_size))\n",
    "                batch_features = []\n",
    "                \n",
    "                for b in range(num_batches):\n",
    "                    start = b * self.batch_size\n",
    "                    end = min((b + 1) * self.batch_size, num_crops)\n",
    "                    input_data = self._preprocess_batch(crops[start:end])\n",
    "                    output = self.session.infer([input_data.copy()])[0]\n",
    "                    batch_features.append(output[:end-start])\n",
    "                \n",
    "                if batch_features:\n",
    "                    features = np.vstack(batch_features)\n",
    "                else:\n",
    "                    features = np.array([])\n",
    "            else:\n",
    "                features = np.array([])\n",
    "        \n",
    "        detections = []\n",
    "        for i, (box, conf) in enumerate(zip(bbox_xywh, confidences)):\n",
    "            if i < len(features):\n",
    "                detections.append(Detection(box, conf, features[i].flatten()))\n",
    "\n",
    "        self.tracker.predict()\n",
    "        self.tracker.update(detections)\n",
    "\n",
    "        outputs = []\n",
    "        for track in self.tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1: continue\n",
    "            box = track.to_tlwh()\n",
    "            x1, y1, x2, y2 = int(box[0]), int(box[1]), int(box[0]+box[2]), int(box[1]+box[3])\n",
    "            outputs.append([x1, y1, x2, y2, track.track_id])\n",
    "            \n",
    "        return np.array(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9292fc-f49c-410d-8bf3-08069171c2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, cfg, bgr2rgb=True):\n",
    "    \"\"\"图片预处理\"\"\"\n",
    "    img, scale_ratio, pad_size = letterbox(image, new_shape=cfg['input_shape'])\n",
    "    if bgr2rgb:\n",
    "        img = img[:, :, ::-1]\n",
    "    img = img.transpose(2, 0, 1)  # HWC2CHW\n",
    "    img = np.ascontiguousarray(img, dtype=np.float32)\n",
    "    img /= 255.0  # 关键修改：归一化 0-255 -> 0.0-1.0\n",
    "    return img, scale_ratio, pad_size\n",
    "\n",
    "\n",
    "def draw_bbox(bbox, img0, color, wt, names):\n",
    "    \"\"\"在图片上画预测框\"\"\"\n",
    "    det_result_str = ''\n",
    "    for idx in range(bbox.shape[0]):\n",
    "        # 关键修改：yolov5-face NMS后，index 5 是类别\n",
    "        class_id = int(bbox[idx][5]) \n",
    "        \n",
    "        # 关键修改：修复之前的语法错误 float(bool) -> float(val)\n",
    "        if float(bbox[idx][4]) < 0.05:\n",
    "            continue\n",
    "        \n",
    "        x1, y1, x2, y2 = int(bbox[idx][0]), int(bbox[idx][1]), int(bbox[idx][2]), int(bbox[idx][3])\n",
    "        center_x = (x1 + x2) // 2\n",
    "        center_y = (y1 + y2) // 2\n",
    "\n",
    "        # 输出人脸框中心点和四角坐标\n",
    "        \n",
    "        img0 = cv2.rectangle(img0, (int(bbox[idx][0]), int(bbox[idx][1])), (int(bbox[idx][2]), int(bbox[idx][3])),\n",
    "                             color, wt)\n",
    "        \n",
    "        label_name = names[int(class_id)] if names and len(names) > class_id else 'face'\n",
    "        img0 = cv2.putText(img0, str(idx) + ' ' + label_name, (int(bbox[idx][0]), int(bbox[idx][1] + 16)),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "        img0 = cv2.putText(img0, '{:.4f}'.format(bbox[idx][4]), (int(bbox[idx][0]), int(bbox[idx][1] + 32)),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "        \n",
    "        # 关键修改：关键点从 index 6 开始\n",
    "        if bbox.shape[1] >= 16:\n",
    "            for k in range(5):\n",
    "                kpt_x = int(bbox[idx][6 + 2 * k])\n",
    "                kpt_y = int(bbox[idx][6 + 2 * k + 1])\n",
    "                cv2.circle(img0, (kpt_x, kpt_y), 3, (255, 0, 0), -1)\n",
    "\n",
    "        det_result_str += '{} {} {} {} {} {}\\n'.format(\n",
    "           label_name, str(bbox[idx][4]), bbox[idx][0], bbox[idx][1], bbox[idx][2], bbox[idx][3])\n",
    "    \n",
    "    return img0\n",
    "\n",
    "\n",
    "def get_labels_from_txt(path):\n",
    "    # ...existing code...\n",
    "    labels_dict = dict()\n",
    "    with open(path) as f:\n",
    "        for cat_id, label in enumerate(f.readlines()):\n",
    "            labels_dict[cat_id] = label.strip()\n",
    "    return labels_dict\n",
    "\n",
    "\n",
    "def draw_prediction(pred, image, labels):\n",
    "    # ...existing code...\n",
    "    imgbox = widgets.Image(format='jpg', height=720, width=1280)\n",
    "    img_dw = draw_bbox(pred, image, (0, 255, 0), 2, labels)\n",
    "    imgbox.value = cv2.imencode('.jpg', img_dw)[1].tobytes()\n",
    "    display(imgbox)\n",
    "\n",
    "\n",
    "def infer_image(img_path, model, class_names, cfg):\n",
    "    \"\"\"图片推理\"\"\"\n",
    "    image = cv2.imread(img_path)\n",
    "    img, scale_ratio, pad_size = preprocess_image(image, cfg)\n",
    "    \n",
    "    # 关键修改：增加 Batch 维度 (3,640,640) -> (1,3,640,640)\n",
    "    if len(img.shape) == 3:\n",
    "        img = img[None] \n",
    "\n",
    "    output = model.infer([img])[0]\n",
    "    output = torch.tensor(output)\n",
    "    \n",
    "    # NMS nm=10 保留关键点\n",
    "    boxout = nms(output, conf_thres=cfg[\"conf_thres\"], iou_thres=cfg[\"iou_thres\"], nm=10)\n",
    "    pred_all = boxout[0].numpy()\n",
    "    \n",
    "    # 坐标还原\n",
    "    scale_coords(cfg['input_shape'], pred_all, image.shape, ratio_pad=(scale_ratio, pad_size))\n",
    "    if pred_all.shape[1] >= 16:\n",
    "        # 关键点还原，传入 index 6:16\n",
    "        pred_all[:, 6:16] = scale_coords_landmarks(cfg['input_shape'], pred_all[:, 6:16], image.shape, ratio_pad=(scale_ratio, pad_size))\n",
    "        \n",
    "    draw_prediction(pred_all, image, class_names)\n",
    "\n",
    "def draw_tracked_bbox(img, bbox, identities=None, offset=(0, 0)):\n",
    "    for i, box in enumerate(bbox):\n",
    "        x1, y1, x2, y2 = [int(i) for i in box]\n",
    "        x1 += offset[0]\n",
    "        x2 += offset[0]\n",
    "        y1 += offset[1]\n",
    "        y2 += offset[1]\n",
    "        \n",
    "        # 计算中心点\n",
    "        center_x = (x1 + x2) // 2\n",
    "        center_y = (y1 + y2) // 2\n",
    "        \n",
    "        id = int(identities[i]) if identities is not None else 0\n",
    "        \n",
    "        # 绘制矩形框\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # 绘制 ID 和 中心点\n",
    "        label = f'ID:{id} ({center_x},{center_y})'\n",
    "        t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n",
    "        cv2.rectangle(img, (x1, y1), (x1 + t_size[0] + 3, y1 + t_size[1] + 4), (0, 255, 0), -1)\n",
    "        cv2.putText(img, label, (x1, y1 + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1)\n",
    "        \n",
    "        # # 可以在这里打印或发送串口数据\n",
    "        # print(f\"Track ID: {id}, Center: {center_x}, {center_y}\")\n",
    "        \n",
    "    return img\n",
    "\n",
    "\n",
    "def infer_frame_with_vis(image, model, labels_dict, cfg, bgr2rgb=True):\n",
    "    img, scale_ratio, pad_size = preprocess_image(image, cfg, bgr2rgb)\n",
    "    \n",
    "    # 关键修改：增加 Batch 维度\n",
    "    if len(img.shape) == 3:\n",
    "        img = img[None]\n",
    "\n",
    "    output = model.infer([img])[0]\n",
    "    output = torch.tensor(output)\n",
    "    \n",
    "    boxout = nms(output, conf_thres=cfg[\"conf_thres\"], iou_thres=cfg[\"iou_thres\"], nm=10)\n",
    "    pred_all = boxout[0].numpy()\n",
    "    \n",
    "    scale_coords(cfg['input_shape'], pred_all, image.shape, ratio_pad=(scale_ratio, pad_size))\n",
    "    # if pred_all.shape[1] >= 16:\n",
    "        # pred_all[:, 6:16] = scale_coords_landmarks(cfg['input_shape'], pred_all[:, 6:16], image.shape, ratio_pad=(scale_ratio, pad_size))\n",
    "        \n",
    "    # img_vis = draw_bbox(pred_all, image, (0, 255, 0), 2, labels_dict)\n",
    "\n",
    "    xywh_bboxs = []\n",
    "    confs = []\n",
    "    clses = []\n",
    "    if pred_all.shape[0] > 0:\n",
    "        # 1. 数据格式转换：(x1, y1, x2, y2) -> (x, y, w, h)\n",
    "        # pred_all 每一行是 [x1, y1, x2, y2, conf, class, ...]\n",
    "        for i in range(pred_all.shape[0]):\n",
    "            x1, y1, x2, y2 = pred_all[i, :4]\n",
    "            conf = pred_all[i, 4]\n",
    "            cls = pred_all[i, 5]\n",
    "            \n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "            # DeepSort 需要左上角 x,y 和 w,h\n",
    "            xywh_bboxs.append([x1, y1, w, h])\n",
    "            confs.append(conf)\n",
    "            clses.append(cls)\n",
    "            \n",
    "        xywh_bboxs = torch.Tensor(xywh_bboxs)\n",
    "        confs = torch.Tensor(confs)\n",
    "        clses = torch.Tensor(clses)\n",
    "        \n",
    "        # 2. 更新 DeepSort\n",
    "        # 传入原始 image 用于特征提取\n",
    "        outputs = deepsort.update(xywh_bboxs, confs, clses, image)\n",
    "        \n",
    "        # [修改] 增加类型检查和转换\n",
    "        # 某些版本的 DeepSort 可能返回 list，需要转为 numpy array\n",
    "        if isinstance(outputs, list):\n",
    "            outputs = np.array(outputs)\n",
    "            \n",
    "        # 3. 绘制跟踪结果\n",
    "        # 确保 outputs 是数组且不为空\n",
    "        if outputs is not None and len(outputs) > 0:\n",
    "            # 再次检查是否是元组（有些版本返回 (bbox, id)）\n",
    "            if isinstance(outputs, tuple):\n",
    "                # 如果是元组，通常意味着没有检测到或者格式不同，视具体库而定\n",
    "                # 这里假设如果是元组就是空的或者无效的\n",
    "                img_vis = image\n",
    "            else:\n",
    "                bbox_xyxy = outputs[:, :4]\n",
    "                identities = outputs[:, -1]\n",
    "                img_vis = draw_tracked_bbox(image, bbox_xyxy, identities)\n",
    "        else:\n",
    "            img_vis = image\n",
    "    else:\n",
    "        img_vis = image\n",
    "    # --- DeepSort 集成结束 ---\n",
    "\n",
    "    # 如果你还想保留关键点显示，可以把原有的 draw_bbox 逻辑加回来，或者合并逻辑\n",
    "    # 但通常跟踪后主要关注 ID 和位置\n",
    "    return img_vis\n",
    "\n",
    "\n",
    "def img2bytes(image):\n",
    "    \"\"\"将图片转换为字节码\"\"\"\n",
    "    return bytes(cv2.imencode('.jpg', image)[1])\n",
    "\n",
    "\n",
    "def infer_video(video_path, model, labels_dict, cfg):\n",
    "    \"\"\"视频推理\"\"\"\n",
    "    image_widget = widgets.Image(format='jpeg', width=800, height=600)\n",
    "    display(image_widget)\n",
    "\n",
    "    # 读入视频\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    prev_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        ret, img_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # 对视频帧进行推理\n",
    "        image_pred = infer_frame_with_vis(img_frame, model, labels_dict, cfg, bgr2rgb=True)\n",
    "        \n",
    "                # 计算 FPS\n",
    "        curr_time = time.time()\n",
    "        exec_time = curr_time - prev_time\n",
    "        prev_time = curr_time\n",
    "        fps = 1 / exec_time if exec_time > 0 else 0\n",
    "        \n",
    "        # 在画面上绘制 FPS\n",
    "        cv2.putText(image_pred, f\"FPS: {fps:.2f}\", (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        \n",
    "        image_widget.value = img2bytes(image_pred)\n",
    "\n",
    "\n",
    "def infer_camera(model, labels_dict, cfg):\n",
    "    \"\"\"外设摄像头实时推理\"\"\"\n",
    "    def find_camera_index():\n",
    "        max_index_to_check = 10  # Maximum index to check for camera\n",
    "\n",
    "        for index in range(max_index_to_check):\n",
    "            cap = cv2.VideoCapture(index)\n",
    "            if cap.read()[0]:\n",
    "                cap.release()\n",
    "                return index\n",
    "\n",
    "        # If no camera is found\n",
    "        raise ValueError(\"No camera found.\")\n",
    "\n",
    "    # 获取摄像头\n",
    "    camera_index = find_camera_index()\n",
    "    cap = cv2.VideoCapture(camera_index)\n",
    "    # 初始化可视化对象\n",
    "    image_widget = widgets.Image(format='jpeg', width=1280, height=720)\n",
    "\n",
    "    # FPS 计算初始化\n",
    "    prev_time = time.time()\n",
    "\n",
    "    display(image_widget)\n",
    "    while True:\n",
    "        # 对摄像头每一帧进行推理和可视化\n",
    "        _, img_frame = cap.read()\n",
    "        image_pred = infer_frame_with_vis(img_frame, model, labels_dict, cfg)\n",
    "        \n",
    "         # 计算 FPS\n",
    "        curr_time = time.time()\n",
    "        exec_time = curr_time - prev_time\n",
    "        prev_time = curr_time\n",
    "        fps = 1 / exec_time if exec_time > 0 else 0\n",
    "        \n",
    "        # 在画面上绘制 FPS\n",
    "        cv2.putText(image_pred, f\"FPS: {fps:.2f}\", (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        print(f\"FPS: {fps:.2f}\")\n",
    "        image_widget.value = img2bytes(image_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e455e8d",
   "metadata": {},
   "source": [
    "# 样例运行\n",
    "\n",
    "* 初始化相关参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccded13-fad1-4b88-b63f-d46deb23aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'conf_thres': 0.4,  # 模型置信度阈值，阈值越低，得到的预测框越多\n",
    "    'iou_thres': 0.5,  # IOU阈值，高于这个阈值的重叠预测框会被过滤掉\n",
    "    'input_shape': [640, 640],  # 模型输入尺寸\n",
    "}\n",
    "\n",
    "model_path = 'yolo_face.om'\n",
    "# 初始化推理模型\n",
    "model = InferSession(0, model_path)\n",
    "labels_dict = {0: 'face'}\n",
    "\n",
    "deepsort = DeepSort(\n",
    "    model_path='deepsort_mars.om',\n",
    "    max_dist=0.2, \n",
    "    min_confidence=0.3, \n",
    "    nms_max_overlap=0.5, \n",
    "    max_iou_distance=0.7, \n",
    "    max_age=70, \n",
    "    n_init=3, \n",
    "    nn_budget=100, \n",
    "    use_cuda=False \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e9cf34",
   "metadata": {},
   "source": [
    "* 选择推理模式。\"infer_mode\"有三个取值：image, camera, video，分别对应图片推理、摄像头实时推理和视频推理。默认使用视频推理模式。\n",
    "* 我们选取的样例是一个赛车视频，执行下面的代码后可以看到模型会对视频的每一帧进行推理，并将预测结果展示在画面上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aab68b6b-7d65-4dd9-ab0b-c5c9733c415a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce1d55043f842a69f1a4b12bbe86de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpeg', height='720', width='1280')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4187860/3350929844.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minfer_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0minfer_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'camera'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0minfer_camera\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0minfer_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'video'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvideo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'racing.mp4'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4187860/3522527725.py\u001b[0m in \u001b[0;36minfer_camera\u001b[0;34m(model, labels_dict, cfg)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;31m# 对摄像头每一帧进行推理和可视化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mimage_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer_frame_with_vis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mimage_widget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg2bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4187860/3522527725.py\u001b[0m in \u001b[0;36minfer_frame_with_vis\u001b[0;34m(image, model, labels_dict, cfg, bgr2rgb)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/lib/python3.9/site-packages/ais_bench/infer/interface.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(self, feeds, mode, custom_sizes)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'dymdims'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_dynamic_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdyshapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMemorySummary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/lib/python3.9/site-packages/ais_bench/infer/interface.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feeds, out_array)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout_array\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# convert to host tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "infer_mode = 'camera'  ## 如果修改为'camera' 则使用摄像头进行推理\n",
    "\n",
    "if infer_mode == 'image':\n",
    "    img_path = 'ImageforInfer.jpg'\n",
    "    infer_image(img_path, model, labels_dict, cfg)\n",
    "elif infer_mode == 'camera':\n",
    "    infer_camera(model, labels_dict, cfg)\n",
    "elif infer_mode == 'video':\n",
    "    video_path = 'racing.mp4'\n",
    "    infer_video(video_path, model, labels_dict, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27887b77",
   "metadata": {},
   "source": [
    "# 样例总结与扩展\n",
    "以上就是这个样例的全部内容了，值得关注的是在模型推理后有一步非常重要的后处理，就是非极大值抑制，即NMS，由于模型的原始预测结果会有非常多无效或重叠的预测框，我们需要通过NMS来进行过滤。再者，模型预测框的表示往往是一个标准化的结果，比如0到1之间，我们需要通过坐标转换将结果与原始图片的宽高对应上。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

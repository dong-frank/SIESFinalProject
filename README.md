# 直播机器人

1. NPU设备上端到端推理的成功尝试
  在本项目的实施过程中，团队注意到市面上多数嵌入式设备仅配备CPU，算力较为有限，难以满足人脸识别算法的基础性能需求，导致识别速度过慢。经过深入调研与讨论，团队最终选用搭载Ascend 310B NPU推理芯片的Atlas 200I DK A2作为人脸识别与追踪的算力平台，并成功实现了YOLOv5-face + DeepSORT算法的端到端推理部署。在480P单人脸视频流测试中，AICore占用率约为40%，峰值功耗控制在10W以内，在画面中无人脸出现时帧数约为15fps，画面中出现单张人脸后约为9fps，与在GPU上运行相比，算法准确度差异不超过0.05%，算法的速度和准确度都符合使用条件。
2. 改进DeepSORT算法，针对实际项目中的应用场景提出目标追回算法，有效提升目标跟踪率
  针对 DeepSort 在复杂场景下可能出现的 ID 跳变或丢失问题，本项目引入了一套基于 IoU（交并比）的位置接力机制，实现了“一旦锁定，持续追踪”的稳定效果。其核心原理在于系统会持续记忆被锁定目标在上一帧中的位置。当在新的一帧中发现原跟踪 ID 突然消失时，系统并不会立即丢失目标，而是计算上一帧目标框与当前帧所有新检测框之间的 IoU。若某个新检测框与历史位置的交并比超过设定阈值（如 0.3），则系统判定其为同一目标，并自动将锁定 ID 切换至这个新编号，从而在跟踪逻辑上实现无缝衔接，确保舵机控制不会因 ID 跳变而中断。
3. 直播监看平台的搭建，支持Web端对直播机器人的控制
  为确保第2点所述的目标追回算法在实际场景中得以有效应用，必须通过用户与系统的实时交互来指定并锁定待追踪的人脸目标。为此，我们搭建了一套 Web 端直播监看平台，该平台不仅提供实时画面预览、人脸目标选定、直播录像保存及摄像头开关等核心功能，还实现了对追踪过程的直观干预与灵活控制，从而为人脸跟踪算法的稳定运行提供了必要的人机交互基础。
4. PID算法控制云台运动
  在云台伺服控制方面，我们部署了PID控制算法。经过细致的参数调试与优化，系统实现了低超调、快响应的高性能跟踪，为视觉目标的持续稳定锁定提供了可靠保障。
5. NPU设备作为上位机和arduino设备作为下位机的协同控制方案得到成功验证
  为弥补 Atlas 200I DK A2 在直接硬件控制方面的不足（如无法驱动云台），团队确定了“上位机决策+下位机执行”的协同架构：由 NPU 设备作为上位机，专注视觉推理与跟踪决策；同时选用开发简便、生态成熟的 Arduino 兼容设备 StarDuino 作为下位机，专责云台运动控制。该方案经实际验证，不仅实现了视觉与机械系统的可靠联动，更凭借高效的通信与任务分工，确保了整体控制链路的高响应与低延时。
6. 实物原型搭建
  在指导老师的专业支持下，团队协同工作，最终完成了集成NPU推理、跟踪算法与运动控制系统的实物原型搭建。
7. 开源贡献
  本项目在研发过程中，尤其在NPU环境搭建与DeepSORT算法部署等环节，得益于开源社区的宝贵资源。为践行开源精神并回馈社区，我们决定将项目全部代码开源，重点贡献在Atlas 200I DK A2设备上实现端到端推理的核心代码。